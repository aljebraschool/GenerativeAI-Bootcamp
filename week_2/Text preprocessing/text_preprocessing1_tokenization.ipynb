{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ffce95",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Machine Learning - Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa174c",
   "metadata": {},
   "source": [
    "### Tokenization - Converting paragraph (corpus) into sentences (document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325fd66",
   "metadata": {},
   "source": [
    "Tokenization is a preprocessing technique used to transform human-readable text into a format that computers can process. \n",
    "\n",
    "It involves breaking down text into smaller units, such as sentences or words, making the data easier for models to analyze and process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcdb533",
   "metadata": {},
   "source": [
    "Tokenization can be applied at different levels:\n",
    "\n",
    "- Sentence Tokenization: Splitting a paragraph or corpus into individual sentences.\n",
    "\n",
    "- Word Tokenization: Further breaking down each sentence into individual words.\n",
    "\n",
    "\n",
    "For example, consider the following paragraph (corpus):\n",
    "\n",
    "\"The era of smartphones has revolutionized how we generate ideas. Writing is now assisted by intelligent tools that help complete our sentences.\"\n",
    "\n",
    "Sentence Tokenization would split this into:\n",
    "\n",
    "\"The era of smartphones has revolutionized how we generate ideas.\"\n",
    "\n",
    "\"Writing is now assisted by intelligent tools that help complete our sentences.\"\n",
    "\n",
    "Word Tokenization would break the first sentence into:\n",
    "\n",
    "\"The\", \"era\", \"of\", \"smartphones\", \"has\", \"revolutionized\", \"how\", \"we\", \"generate\", \"ideas.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a472e",
   "metadata": {},
   "source": [
    "> This structured representation of text is crucial for feeding data into the transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3922670",
   "metadata": {},
   "source": [
    "## Tokenization code Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5de089",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "  Hello my name is Ridwan Ibidunni. I am a graduate of mathematics with a keen interest in developing AI applications\n",
    "    to solve problems in our environments. I love coding and teaching people how to code.\n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408767e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5129a098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Hello my name is Ridwan Ibidunni. I am a graduate of mathematics with a keen interest in developing AI applications\n",
      "    to solve problems in our environments. I love coding and teaching people how to code.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bde5f0",
   "metadata": {},
   "source": [
    "### Tokenizing paragraph to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7134dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.1.6)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m70.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.7/791.7 kB\u001b[0m \u001b[31m31.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.5.2 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ae7619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/aljebra/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # type: ignore\n",
    "# Download the 'punkt_tab' data package\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2df03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import then method needed to tokenize sentence from nltk\n",
    "from nltk import sent_tokenize\n",
    "document = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32daca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f40cc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Hello my name is Ridwan Ibidunni.\n",
      "I am a graduate of mathematics with a keen interest in developing AI applications\n",
      "    to solve problems in our environments.\n",
      "I love coding and teaching people how to code.\n"
     ]
    }
   ],
   "source": [
    "for item in document:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9ca35",
   "metadata": {},
   "source": [
    "### Tokenizing paragraph into word. And sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06a68ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Ridwan',\n",
       " 'Ibidunni',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'graduate',\n",
       " 'of',\n",
       " 'mathematics',\n",
       " 'with',\n",
       " 'a',\n",
       " 'keen',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'developing',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'problems',\n",
       " 'in',\n",
       " 'our',\n",
       " 'environments',\n",
       " '.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'coding',\n",
       " 'and',\n",
       " 'teaching',\n",
       " 'people',\n",
       " 'how',\n",
       " 'to',\n",
       " 'code',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "#convert paragraph to word\n",
    "corpus_tokenized_word = word_tokenize(corpus)\n",
    "\n",
    "corpus_tokenized_word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cfbc04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'my', 'name', 'is', 'Ridwan', 'Ibidunni', '.']\n",
      "['I', 'am', 'a', 'graduate', 'of', 'mathematics', 'with', 'a', 'keen', 'interest', 'in', 'developing', 'AI', 'applications', 'to', 'solve', 'problems', 'in', 'our', 'environments', '.']\n",
      "['I', 'love', 'coding', 'and', 'teaching', 'people', 'how', 'to', 'code', '.']\n"
     ]
    }
   ],
   "source": [
    "#convert sentence to word\n",
    "for word in document:\n",
    "    print(word_tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e0fd775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Ridwan',\n",
       " 'Ibidunni',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'graduate',\n",
       " 'of',\n",
       " 'mathematics',\n",
       " 'with',\n",
       " 'a',\n",
       " 'keen',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'developing',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'problems',\n",
       " 'in',\n",
       " 'our',\n",
       " 'environments',\n",
       " '.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'coding',\n",
       " 'and',\n",
       " 'teaching',\n",
       " 'people',\n",
       " 'how',\n",
       " 'to',\n",
       " 'code',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use this library to have more detailed separation of the puntuation\n",
    "from nltk import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a1b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
